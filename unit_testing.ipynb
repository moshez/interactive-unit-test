{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unit testing tutorial\n",
    "\n",
    "This unit test tutorial takes place inside a Jupyter notebook.\n",
    "All tests are real.\n",
    "The only thing we have to do is to write a little \"test runner\".\n",
    "Usually, you will use something like `pytest` to run your tests.\n",
    "However, here we run our tests inside a notebook.\n",
    "\n",
    "Every single test is supposed to fail!\n",
    "The only interesting thing about tests is how they fail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest\n",
    "\n",
    "def test(klass):\n",
    "    loader = unittest.TestLoader()\n",
    "    suite=loader.loadTestsFromTestCase(klass)\n",
    "    runner = unittest.TextTestRunner()\n",
    "    runner.run(suite)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tests can *fail* or *error*. These are different things.\n",
    "A test error happens when an exception is raised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestSimpleFailure(unittest.TestCase):\n",
    "    def test_fail(self):\n",
    "        1/0\n",
    "\n",
    "test(TestSimpleFailure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A test *failure* happens when a test assertion is violated.\n",
    "This means that to fail tests,\n",
    "we should write assertions.\n",
    "The `unittest` library comes with its own assertions,\n",
    "but those are a bit rigid and unpleasant to use.\n",
    "\n",
    "`pytest` offers a mini-language that looks like Python:\n",
    "it will look at your `assert` statements,\n",
    "and generate assertions from that.\n",
    "This can sometimes be obscure:\n",
    "when it works, it works great,\n",
    "but when it violates expectations,\n",
    "it is somewhat obscure why.\n",
    "\n",
    "In contract, `PyHamcrest` is a flexible assertion library.\n",
    "It does not try to be a unit test framework\n",
    "*or*\n",
    "a unit test runner.\n",
    "Its only goal is to be the best assertion library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hamcrest import assert_that, equal_to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestEquality(unittest.TestCase):\n",
    "    def test_paradox(self):\n",
    "        assert_that(1, equal_to(0))\n",
    "\n",
    "test(TestEquality)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often it turns out we need to construct some object the same way for multiple tests.\n",
    "Such\n",
    "*fixtures*\n",
    "can be constructed using `setUp`.\n",
    "In this example,\n",
    "we build the object `1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestMultipleEquality(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        self.value = 1\n",
    "    def test_paradox(self):\n",
    "        assert_that(self.value, equal_to(0))\n",
    "    def test_anoter_paradox(self):\n",
    "        assert_that(self.value, equal_to(2))\n",
    "\n",
    "test(TestMultipleEquality)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some fixtures need\n",
    "*cleanup*.\n",
    "For example,\n",
    "files should be closed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestRead(unittest.TestCase):\n",
    "    def test_read(self):\n",
    "        self.addCleanup(lambda: fpin.close())\n",
    "        fpin = open(\"/dev/zero\")\n",
    "        assert_that(fpin.read(4), equal_to(\"\"))\n",
    "\n",
    "test(TestRead)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we need a cleanup-needing fixture for multiple tests,\n",
    "we can move the creation/cleanup logic to `setUp`.\n",
    "This is nothing more than what we already know:\n",
    "`addCleanup` runs at the end of a test,\n",
    "and `setUp` runs before each test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDoubleRead(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        self.addCleanup(lambda: self.fpin.close())\n",
    "        self.fpin = open(\"/dev/zero\")\n",
    "        \n",
    "    def test_short_read(self):\n",
    "        assert_that(self.fpin.read(4), equal_to(\"\"))\n",
    "        \n",
    "    def test_long_read(self):\n",
    "        assert_that(self.fpin.read(8), equal_to(\"\"))        \n",
    "\n",
    "test(TestDoubleRead)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the tests are so similar as to only differ by a parameter,\n",
    "we can use\n",
    "*subtests*.\n",
    "Subtests share all the logic,\n",
    "but they do not share failures.\n",
    "Note that a subtest failing does not halt execution of the test function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestMultiRead(unittest.TestCase):\n",
    "    def test_all_reads(self):\n",
    "        self.addCleanup(lambda: fpin.close())\n",
    "        fpin = open(\"/dev/zero\")\n",
    "        for length in (1, 4, 8):\n",
    "            with self.subTest(length=length):\n",
    "                assert_that(fpin.read(length), equal_to(\"\"))\n",
    "\n",
    "test(TestMultiRead)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in this case,\n",
    "the same open file was used in all tests.\n",
    "If we want separate fixtures for subtests,\n",
    "we need to create them separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestMultiSeparateRead(unittest.TestCase):\n",
    "    def test_all_reads(self):\n",
    "        for length in (1, 4, 8):\n",
    "            with self.subTest(length=length):\n",
    "                with open(\"/dev/zero\") as fpin:\n",
    "                    assert_that(fpin.read(length), equal_to(\"\"))\n",
    "\n",
    "test(TestMultiSeparateRead)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyHamcrest has a lot of useful matchers. Here are some of them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hamcrest import all_of, close_to, contains_string, has_items\n",
    "\n",
    "class TestInterestingMatchers(unittest.TestCase):\n",
    "    def test_close_to(self):\n",
    "        assert_that(0.1 + 0.2, close_to(0.4, 0.05))\n",
    "    def test_contains_string(self):\n",
    "        assert_that(\"hello\", contains_string(\"lll\"))\n",
    "    def test_logical(self):\n",
    "        assert_that(\"hello\", all_of(equal_to(\"hello\"), equal_to(\"goodbye\")))\n",
    "    def test_combined(self):\n",
    "        assert_that([0.1 + 0.2], has_items(close_to(0.3, 0.05), close_to(0.4, 0.05)))\n",
    "\n",
    "test(TestInterestingMatchers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One powrful way to build projects is by using the `mock` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unittest import mock\n",
    "from hamcrest import calling, raises\n",
    "\n",
    "class TestMockingConstructs(unittest.TestCase):\n",
    "    def test_simple_mock(self):\n",
    "        my_obj = mock.Mock()\n",
    "        my_obj.some_attribute = 1\n",
    "        assert_that(my_obj.some_attribute, equal_to(2))\n",
    "    def test_call_args(self):\n",
    "        my_obj = mock.Mock()\n",
    "        my_obj.write(\"hello\")\n",
    "        expected = mock.call(\"goodbye\")[1:]\n",
    "        assert_that(list(my_obj.write.call_args), equal_to(expected))\n",
    "    def test_side_effect(self):\n",
    "        my_obj = mock.Mock()\n",
    "        my_obj.side_effect = [1, 2, 3]\n",
    "        for val in range(3):\n",
    "            with self.subTest(val=val):\n",
    "                assert_that(my_obj(), equal_to(val))\n",
    "    def test_expected_exception(self):\n",
    "        my_obj = mock.Mock()\n",
    "        my_obj.side_effect = ValueError(\"nah\")\n",
    "        assert_that(calling(my_obj), raises(IOError))\n",
    "test(TestMockingConstructs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
